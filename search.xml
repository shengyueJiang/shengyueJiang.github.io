<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>流畅的Python-第二章-学习笔记</title>
      <link href="/2021/12/11/%E6%B5%81%E7%95%85%E7%9A%84Python-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/12/11/%E6%B5%81%E7%95%85%E7%9A%84Python-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="序列构成的数组"><a href="#序列构成的数组" class="headerlink" title="序列构成的数组"></a>序列构成的数组</h1><h2 id="内置序列类型概览"><a href="#内置序列类型概览" class="headerlink" title="内置序列类型概览"></a>内置序列类型概览</h2><ul><li><p>python标准库用C实现了丰富的序列类型，列举如下</p><ul><li><p>容器序列</p><ul><li>list，tuple和collections.deque这些序列能存放不类型的数据。</li></ul></li><li><p>扁平序列</p><ul><li>str，bytes，bytearray，memoryview和arry.arry，这类序列只能容纳一种类型。</li></ul><blockquote><p>容器序列存放的是它们所包含的任意类型的对象的引用，而扁平对象中存储的是值而不是引用。</p></blockquote></li></ul></li><li><p>可以以按照能否被修改来分类：</p><ul><li>可变序列<ul><li>list，byterarray，array.array，collections.deque和memoryview。</li></ul></li><li>不可变序列<ul><li>tuple，str和bytes</li></ul></li></ul></li><li><p>下面是可变序列和不可变序列的差异UML图：</p><p><img src alt></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 流畅的Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>流畅的Python-第一章-学习笔记</title>
      <link href="/2021/12/11/%E6%B5%81%E7%95%85%E7%9A%84Python-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/12/11/%E6%B5%81%E7%95%85%E7%9A%84Python-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="python数据类型"><a href="#python数据类型" class="headerlink" title="python数据类型"></a>python数据类型</h1><ul><li><p>不管在那种框架下书写程序，我们都会花费大量的时间去实现那些会被框架本身调用的方法。</p></li><li><p>python在碰到特殊的句法的时候，会使用特殊方法去激活一些基本的对象操作，这些特殊方法的名字以两个下划线开头，以两个下划线结尾。</p><blockquote><p>例如当我们使用某种根据索引找到值的语法时，实际上python是在调用一个名为<strong>getitem</strong>的方法，为了实际求得某值实际上，解释器会调用这个方法来完成。</p></blockquote></li><li><p>这些特殊的方法可以让我们自己的对象实现和支持以下的语言架构，并且与其交互：</p><ul><li>迭代</li><li>集合类</li><li>属性访问</li><li>运算符重载</li><li>函数和方法的调用</li><li>对象的创建和销毁</li><li>字符串表示形式和格式化</li><li>管理上下文</li></ul><blockquote><p>博主理解：在我们要实现某个类的时候，我们有时候会花费大量的时间来进行python已经准备好的功能，这会增加我们的工作量，做重复的共做，一旦我们使用了魔术方法，我们就可以赋予我们自己的对象python的某些内置功能，包括但不限于索引等。</p></blockquote></li></ul><h2 id="一摞python风格的纸牌"><a href="#一摞python风格的纸牌" class="headerlink" title="一摞python风格的纸牌"></a>一摞python风格的纸牌</h2><p>此部分作者用十几行代码展示了，python特殊方法的用法，通过这个例子我们可以看到作者大大非常强大。</p><ul><li><p>首先，作者调用了collections的namedtuple方法，本方法旨在于构建只有少数的属性但是没有方法的对象，并且在这两条语句中我们简单的定义了一个纸牌对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line">Card = collections.namedtuple(<span class="string">&quot;Card&quot;</span>, [<span class="string">&#x27;rank&#x27;</span>,<span class="string">&#x27;suit&#x27;</span>])</span><br></pre></td></tr></table></figure><blockquote><p>语法：<br><em>collections.namedtuple(typename, field_names, \</em>, verbose=False, rename=False, module=None)*</p><p>其中：</p><p><code>typename</code>：实际上就是你通过<code>namedtuple</code>创建的一个元组的子类的类名，通过这样的方式我们可以初始化各种各样的实例化元组对象。</p><p><code>field_names</code>：类似于字典的<code>key</code>，在这里定义的元组可以通过这样的<code>key</code>去获取里面对应索引位置的元素值，这样的key可以是列表，也可以是用<code>空格、/和逗号</code>这样的分隔符隔开的字符串。</p><p><code>rename</code>：如果<code>rename</code>指定为<code>True</code>，那么你的<code>field_names</code>里面不能包含有<code>非Python标识符，Python中的关键字以及重复的name</code>，如果有，它会默认给你重命名成‘<code>_index</code>’的样式，这个<code>index</code>表示该<code>name</code>在<code>field_names</code>中的索引，例：<code>[&#39;abc&#39;, &#39;def&#39;, &#39;ghi&#39;, &#39;abc&#39;]</code> 将被转换成<code>[&#39;abc&#39;, &#39;_1&#39;, &#39;ghi&#39;, &#39;_3&#39;]</code>。</p><p><strong>注意:在这里我们定义了一个名为Card的对象，里面有rank和suit两个属性，分别用来存储花色和点数，此处定义的是单个纸牌的对象。</strong></p></blockquote></li><li><p>然后使用很多的魔术方法来定义一整个扑克牌的类定义，首先是整个对象的属性部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Deck</span>:</span></span><br><span class="line">    ranks = [<span class="built_in">str</span>(n) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>,<span class="number">11</span>)) + <span class="built_in">list</span>(<span class="string">&quot;JQKA&quot;</span>)]</span><br><span class="line">    suits = <span class="string">&#x27;hongtao heitao meihua fangpian&#x27;</span>.split()</span><br></pre></td></tr></table></figure><blockquote><p>博主理解:本处的两个定义太那个了，简直就是集大成。rank的列表定义使用了一行的for循环，一行for循环的基本语法格式为：</p><ul><li>[expression for x in L]</li></ul><p>另外在迭代器的构建方面，作者使用了非常简约的两个列表相加的模式，秒啊！（可能也是我太菜了</p><p>在定义suits时，作者使用了split构建字符串列表，常规操作，不过很简约。</p></blockquote></li><li><p>最后就是是就是使用魔术方法来进行方法的书写部分，然后来见证奇迹：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Deck</span>:</span></span><br><span class="line">    ranks = [<span class="built_in">str</span>(n) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">2</span>,<span class="number">11</span>)) + <span class="built_in">list</span>(<span class="string">&quot;JQKA&quot;</span>)]</span><br><span class="line">    suits = <span class="string">&#x27;hongtao heitao meihua fangpian&#x27;</span>.split()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self._cards = [Card(rank, suit) <span class="keyword">for</span> suit <span class="keyword">in</span> self.suits <span class="keyword">for</span> rank <span class="keyword">in</span> self.ranks] + [Card(<span class="string">&#x27;wang&#x27;</span>, suit) <span class="keyword">for</span> suit <span class="keyword">in</span> [<span class="string">&#x27;big&#x27;</span>,<span class="string">&#x27;small&#x27;</span>]]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._cards)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,position</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._cards[position]</span><br></pre></td></tr></table></figure></li><li><p>首先我们通过collections内的namedtuple方法，可以很轻松的得到一个单个纸牌对象：</p><p><img src="/2021/12/11/%E6%B5%81%E7%95%85%E7%9A%84Python-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_1.png" alt></p></li><li><p>通过魔术方法定义的对象使我们可以直接调用<strong>len()</strong>来查看一叠纸牌有多少张：</p><p><img src="/2021/12/11/%E6%B5%81%E7%95%85%E7%9A%84Python-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_2.png" alt></p></li><li><p>同时我们也可以进行诸如，索引，随机选择，切片，迭代等通常我们在python内置数据结构中使用的很多功能，这些都是由<strong>getitem</strong>提供的：</p><p><img src="/2021/12/11/%E6%B5%81%E7%95%85%E7%9A%84Python-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1_3.png" alt></p></li><li><p>我们使用特殊方法的两个好处：</p><ul><li>作为类的用户我们不必再去思考我们造轮子制造出来的函数方法，比如如何取到我们对象的长度，到底我们定义的是什么名字。</li><li>可以更加方便地使用python的标准库进行开发，比如random，我们可以直接把对象作为参数进行调用。</li></ul></li><li><p>仿照作者的例子写了一副麻将牌：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Ma = collections.namedtuple(<span class="string">&#x27;Ma&#x27;</span>, [<span class="string">&#x27;rank&#x27;</span>,<span class="string">&#x27;suit&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mahjong</span>:</span></span><br><span class="line">    ranks = [<span class="built_in">str</span>(n) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line">    suiit = <span class="string">&#x27;tiao bing wan&#x27;</span>.split() * <span class="number">4</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self._Ma = [Ma(rank,suit) <span class="keyword">for</span> rank <span class="keyword">in</span> self.ranks <span class="keyword">for</span> suit <span class="keyword">in</span> self.suiit] + [Ma(rank,<span class="string">&#x27;feng&#x27;</span>) <span class="keyword">for</span> rank <span class="keyword">in</span> <span class="string">&#x27;dong nan xi bei&#x27;</span>.split()*<span class="number">4</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self._Ma)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,position</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._Ma[position]</span><br></pre></td></tr></table></figure></li></ul><h2 id="如何使用特殊方法"><a href="#如何使用特殊方法" class="headerlink" title="如何使用特殊方法"></a>如何使用特殊方法</h2><ul><li>特殊方法的存在其实是为了被python解释器调用的，我们自己并不需要去调用它们。所以我们在书写代码的时候可以直接写<strong>len(x)</strong>而不是<strong>my<em>object.\</em>_len__()</strong>，如果python发现<strong>x</strong>是用户自己定义的对象，那么python会自己调用我们重写的<strong>my<em>object.\</em>_len__()</strong>。</li><li>如果使用python内置的数据结构，python通常会直接调用底层的属性而不是再使用特殊方法，旨在于提高运行速度。</li><li>很多时候特殊方法是隐性调用的，我们无需直接使用特殊方法。</li></ul><h3 id="模拟数值类型"><a href="#模拟数值类型" class="headerlink" title="模拟数值类型"></a>模拟数值类型</h3><ul><li><p>刚才我们使用部分特殊方法实现了一些索引等功能，实际上我们还可以使用部分特殊方法实现我们所需要的数值类型的功能。</p></li><li><p>简单的二维向量对象定义：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">vector</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, x = <span class="number">0</span>, y = <span class="number">0</span></span>):</span></span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;vetor(%r, %r)&#x27;</span>%(self.x, self.y)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__abs__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> hypot(self.x, self.y)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__bool__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">bool</span>(<span class="built_in">abs</span>(self))</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span>(<span class="params">self, other</span>):</span></span><br><span class="line">        x = self.x + other.x</span><br><span class="line">        y = self.y + other.y</span><br><span class="line">        <span class="keyword">return</span> vector(x, y)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__mul__</span>(<span class="params">self,scalar</span>):</span></span><br><span class="line">        <span class="keyword">return</span> vector(self.x * scalar, self.y * scalar)</span><br></pre></td></tr></table></figure></li></ul><h3 id="字符串表示形式"><a href="#字符串表示形式" class="headerlink" title="字符串表示形式"></a>字符串表示形式</h3><ul><li>python中的内置函数__repr__()能把一个对象用字符串的形式表达出来，不然我们直接打印对象时，将会直接打印出地址信息等。</li></ul><h3 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h3><ul><li>我们通过__add<em>_()和\</em>_mul__()为向量对象带来了+和*两个算数运算符。</li><li>注意：我们最终获得的向量对象是新创建出来的，我们并没有对原始的两个对象做操作。</li></ul><h3 id="自定义的布尔值"><a href="#自定义的布尔值" class="headerlink" title="自定义的布尔值"></a>自定义的布尔值</h3><ul><li>我们通过__bool__()改变了bool的方法，在此处我们定义bool()，当向量不为0的时候为真，为0时为假。</li></ul><h3 id="特殊方法表格"><a href="#特殊方法表格" class="headerlink" title="特殊方法表格"></a>特殊方法表格</h3><p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fupload-images.jianshu.io%2Fupload_images%2F13371820-ddd42a4f47406038.png&amp;refer=http%3A%2F%2Fupload-images.jianshu.io&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641815722&amp;t=c221dc71e6c413bf53ad74d0797fa093" alt></p><h2 id="关于len不是普通方法的解释"><a href="#关于len不是普通方法的解释" class="headerlink" title="关于len不是普通方法的解释"></a>关于len不是普通方法的解释</h2><ul><li>在python内置数据结构调用特殊方法的功能时，python将会直接从C结构体中读取对象而不是调用特殊方法，特殊方法只是给我们提供了定义自己的对象的时候的一种方法。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 流畅的Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高级程序设计资料</title>
      <link href="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/"/>
      <url>/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在本部分中，只把老师PPT上的部分实例代码拿了过来，关于基础语法的问题还是需要自己了解，本文主要详细介绍关于数据分析与挖掘部分的知识点。</p><h1 id="实例1：温度转换"><a href="#实例1：温度转换" class="headerlink" title="实例1：温度转换"></a>实例1：温度转换</h1><p><img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/wenduzhuanhuan.png" alt></p><h1 id="实例2：身体质量指数BMI"><a href="#实例2：身体质量指数BMI" class="headerlink" title="实例2：身体质量指数BMI"></a>实例2：身体质量指数BMI</h1><p><img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/shentizhiliang.png" alt></p><h1 id="实例3：圆周率计算"><a href="#实例3：圆周率计算" class="headerlink" title="实例3：圆周率计算"></a>实例3：圆周率计算</h1><p><img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/pai.png" alt></p><h1 id="Pandas的数据分析"><a href="#Pandas的数据分析" class="headerlink" title="Pandas的数据分析"></a>Pandas的数据分析</h1><h2 id="pandas的引用"><a href="#pandas的引用" class="headerlink" title="pandas的引用"></a>pandas的引用</h2><ul><li><strong>import pandas as pd</strong></li><li><strong>form pandas import</strong> *</li></ul><blockquote><p>注意两种引用方法的不同，当我们使用这两种方法引用的时候，我们在编写代码的时候调用函数的写法：</p><ul><li>pd.read()</li><li>read()</li></ul><p>当我在调用相同的函数的时候，第一种引用需要加上库名，但是第二种却不用。</p></blockquote><h2 id="pandas的两个数据类型"><a href="#pandas的两个数据类型" class="headerlink" title="pandas的两个数据类型"></a>pandas的两个数据类型</h2><p>pandas中有两个数据类型分别是：</p><ul><li>Series：一种系列</li><li>DataFrame：一种数据框</li></ul><p>pandas都是基于上述数据类型的各种操作</p><blockquote><p>在这里我们明确一下，Numpy和Pandas两个库的应用的不同方向以免混淆。</p><ul><li>Nupmy：更关注数据结构的表达而不是数据的应用，例如Numpy更加关注矩阵的维数，以及矩阵的秩等各种数据相互间关系的性质。</li><li>Pandas：更关注数据间的操作以及数据的应用的功能。</li></ul></blockquote><h2 id="Series数据类型的详解"><a href="#Series数据类型的详解" class="headerlink" title="Series数据类型的详解"></a>Series数据类型的详解</h2><ul><li><strong>Series是一种一维的结构，类似于一维列表和ndarray中的一维数组，但是功能比他们要更为强大，Series由两部分组成：索引index和数值values。</strong></li></ul><p>​        <img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/series.png" alt></p><ul><li><p>在Series类型中，我们可以自己定义索引的内容，可以在初始化的时候通过传参的方式来进行定义:</p><p><img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/index.png" alt></p></li><li><p>series类型可以从以下类型创建：</p><p><img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/class.png" alt></p></li></ul><h1 id="DataFrame类型详解"><a href="#DataFrame类型详解" class="headerlink" title="DataFrame类型详解"></a>DataFrame类型详解</h1><ul><li><strong>Data更像是表格样式的结构，我们拥有行和列的部分可以通过行列操作数据</strong>：</li></ul><p>​    <img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/data.png" alt></p><ul><li><p>在DataFrame表格中，我们的行列索引和维数都有自己的名字，分别是：</p><p><img src="/2021/12/11/%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%B5%84%E6%96%99/data_index.png" alt></p></li></ul><h1 id="基于Sklearn的数据挖掘"><a href="#基于Sklearn的数据挖掘" class="headerlink" title="基于Sklearn的数据挖掘"></a>基于Sklearn的数据挖掘</h1>]]></content>
      
      
      <categories>
          
          <category> 期末考试复习资料 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高级程序设计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数理统计期末考试题目分析</title>
      <link href="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/"/>
      <url>/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="第一题"><a href="#第一题" class="headerlink" title="第一题"></a>第一题</h1><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_1.png" alt></p><blockquote><p>理解题意，题目给出了8个正态分布，需要我们求出某个概率。</p></blockquote><ul><li><p>首先明确正态分布的概念：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_11.png" alt></p></li><li><p>然后需要知道正态分布的一个性质：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_12.png" alt></p></li><li><p>所以有：<br><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_13.png" alt></p></li><li><p>然后明确卡方分布的概念：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_14.png" alt></p></li><li><p>卡方分布的定义：<br><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_15.png" alt></p></li></ul><blockquote><p>我的理解就是套娃，一堆正态分布的分布就是卡方分布。</p></blockquote><ul><li><p>因为我们已经得到了变换后的式子并发现其属于标准正态分布，所以我们可以将其变换为卡方分布，即：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_16.png" alt></p></li><li><p>从本质上讲我们可以把卡方分布理解为一堆正态分布的分布，所以可以直接从题目的目标下手：<br><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_17.png" alt></p></li><li><p>最后我们要会查表，首先明确一个概念：</p><ul><li>可以简单地理解为，由几个正态分布组成就是自由度为几的卡方分布。</li></ul></li><li><p>卡方分布表格：</p><p><img src="https://appwk.baidu.com/naapi/doc/view?ih=945&amp;o=jpg_6_0_______&amp;iw=803&amp;ix=0&amp;iy=0&amp;aimw=803&amp;rn=1&amp;doc_id=e3a2a9b48bd63186bcebbcdc&amp;pn=1&amp;sign=7e941767ddf58627bda970cc0f612fc8&amp;type=1&amp;app_ver=2.9.8.2&amp;ua=bd_800_800_IncredibleS_2.9.8.2_2.3.7&amp;bid=1&amp;app_ua=IncredibleS&amp;uid=&amp;cuid=&amp;fr=3&amp;Bdi_bear=WIFI&amp;from=3_10000&amp;bduss=&amp;pid=1&amp;screen=800_800&amp;sys_ver=2.3.7" alt></p></li><li><p>因为本题目为8自由度的卡方分布，所以查表第一列为8。</p></li><li><p>然后我们找到和20最相近的值20.09，所以得到p为<strong>0.01</strong>。</p></li></ul><p><strong>整理题目不易，先打赏后看，养成习惯：</strong></p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/dashang.png" alt></p><h1 id="第二题"><a href="#第二题" class="headerlink" title="第二题"></a>第二题</h1><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_2.png" alt></p><blockquote><p> 理解题意，题目给出了12个正态分布，需要我们求出卡方分布和自由度。</p></blockquote><ul><li><p>首先明确正态分布的第二个性质：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_21.png" alt></p></li></ul><blockquote><p>我们可以把此性质推广到多个正态分布，理解为多个正态分布的加减变换。</p></blockquote><ul><li><p>所以有：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_22.png" alt></p></li><li><p>复习一下上一题我们讲到的第一个正态分布的性质：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_12.png" alt></p></li><li><p>我们把它倒过来用一下：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_23.png" alt></p></li><li><p>最后我们要明确卡方分布自由度另一个理解：<strong>卡方分布由几个标准正态分布组成自由度就为几。</strong></p></li><li><p>最后我们返回题设，带入我们得到的式子有：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_24.png" alt></p><blockquote><p>补充解释一下，因为我们的得到了$x_1+x_2+x_3$符合标准正态分布，所以我们把它看作是一个整体，后边的式子也都看作一个整体。</p><p>标准正态分布可以理解为，期望为0，方差为1的分布，也就是$N(0,1)$。</p></blockquote></li><li><p>得到答案：<br><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_25.png" alt></p></li></ul><h1 id="第三题"><a href="#第三题" class="headerlink" title="第三题"></a>第三题</h1><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_3.png" alt></p><ul><li><p>首先我们观察x:</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_31.png" alt></p></li></ul><blockquote><p>此处变换使用了正态分布的两个性质，前面两题都有，忘了自己看一下。</p></blockquote><ul><li>然后我们观察y：</li></ul><p>​    <img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_32.png" alt></p><blockquote><p>在此处我们同样使用了正态分布的性质，但是我们发现，单个Y的表达式分布已经满足标准正态分布，所以一大堆Y（本题是9个）就可以转化为卡方分布。（自由度为9）</p></blockquote><ul><li>最后我们明确t分布的表达式：</li></ul><p>​    <img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_33.png" alt></p><ul><li>由刚才我们的观察可以发现，整个$x_1+…+x_9$是符合标准正态分布的，而一大堆变换后的Y也是符合卡方分布的，所以我们可以想办法把它变换成t分布：</li></ul><p>​    <img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_34.png" alt></p><blockquote><p>上部分的9可以和下半部的变换相消，所以我们可以得到上述式子。</p></blockquote><h1 id="第四题"><a href="#第四题" class="headerlink" title="第四题"></a>第四题</h1><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_4.png" alt></p><ul><li><p>首先明确一个定理：<br><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_41.png" alt></p></li><li><p>通过此定理我们可以得到如下变换：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_42.png" alt></p></li><li><p>然后明确第二个定理：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_43.png" alt></p><blockquote><p> 注意：<script type="math/tex">\theta(x) = 1-\theta(-x)</script></p></blockquote></li><li><p>由此定理我们可以得到：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_44.png" alt></p></li><li><p>通过计算其中的分数，我们可以得到我们需要用来查表的值。</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/b_1.png" alt></p><blockquote><p>查表方法：例如我们算出分数的结果是1.71,则我们在第一列先找到1.7，然后我们找到行标签0.01，即是1.71的查表结果0.9564。</p></blockquote></li><li><p>最后的部分可以通过查表来求解：<br><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic2.zhimg.com%2Fv2-f584977f6a44a62ad26c88214938cfbd_b.jpg&amp;refer=http%3A%2F%2Fpic2.zhimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641355522&amp;t=3f9d62fb099b2eae0943d87b68e56c23" alt></p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_45.png" alt></p></li></ul><h1 id="第五题＆第六题"><a href="#第五题＆第六题" class="headerlink" title="第五题＆第六题"></a>第五题＆第六题</h1><blockquote><p>第五题和第六题属于一种类型题目，在此处只分析一道题目。</p></blockquote><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_5.png" alt></p><p><strong>矩估计和最大似然估计有固定的解题思路和套路，我们先讨论矩估计的解题步骤:</strong></p><ul><li><p>根据题目给出的概率密度函数，计算总体的原点矩（如果只有一个参数只要计算一阶原点矩，如果有两个参数要计算一阶和二阶）。由于有参数这里得到的都是带有参数的式子。如果题目给的是某一个常见的分布，就直接列出相应的原点矩（E(x)）。</p><blockquote><p> 注意：我们并不需要明确的知道原点矩是什么只需要对照固定的公式计算就可以了。</p></blockquote><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_51.png" alt></p><blockquote><p>补充解释：在本题中，概率密度函数已经给出，我们根据所给的范围直接套公式计算原点矩，本质上就是在概率密度的范围对其求x的积分。</p></blockquote></li><li><p>根据题目给出的样本。按照计算样本的原点矩。让总体的原点矩与样本的原点矩相等，解出参数。所得结果即为参数的矩估计值。</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_52.png" alt></p><blockquote><p>同理，我们只需要根据固定的步骤设μ为A，然后根据上一步计算出的表达式解出θ的值即可得到答案。</p><p>最终我们需要把最终的结果A换成答案的形式。</p></blockquote></li></ul><p><strong>最大似然估计的解题步骤：</strong></p><ul><li><p>根据对应概率密度函数计算出似然函数L(x)</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_53.png" alt></p><blockquote><p>此处的似然函数，本质上就是对概率密度函数做一个n次的累积乘法。</p></blockquote></li><li><p>对似然函数L(x)取对数以方便求解。</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_54.png" alt></p></li><li><p>根据参数，对第二步所得的函数求导。如果有多个参数，则分别求偏导。令导数等于0（此时L(x)取到最大值）.求出参数。此时所得结果即为参数的最大似然估计值。</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_55.png" alt></p></li><li><p>最后解出θ的值就是最大似然估计量：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_56.png" alt></p></li></ul><h1 id="第七题＆第八题"><a href="#第七题＆第八题" class="headerlink" title="第七题＆第八题"></a>第七题＆第八题</h1><blockquote><p>两道题属于一种类型题，我们只介绍一种。</p></blockquote><ul><li><p>首先我们明确假设验证的概念：假设检验是先对总体参数提出一个假设值，然后利用样本信息判断这一假设是否成立。</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_7.png" alt></p></li><li><p>计算此类题目首先我们需要计算样本均值，以及标准差。</p></li></ul><blockquote><p>标准差：<img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_71.png" alt></p></blockquote><ul><li>本题中我们计算均值和方差的结果为：</li></ul><p>​    <img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_72.png" alt></p><ul><li><p>我们做如下假设：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_73.png" alt></p></li></ul><blockquote><p>同样的，我们暂时不细究两个东西分别代表什么，我们只需要知道需要做两个假设，$H_0$一般假设为我们已经确定的某个值。</p></blockquote><ul><li><p>我们需要明确t验证的公式：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_74.png" alt></p></li><li><p>最后，我们需要把已知变量带入公式中计算结果：</p></li></ul><p>​        <img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_75.png" alt></p><blockquote><p>此时计算出的就是我们根据小样本估计的值，接下来我们便需要与允许的误差值作比较，误差值需要查表。</p></blockquote><p>​        <img src="https://img-blog.csdnimg.cn/20200320000514397.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RIVUNoaW5h,size_16,color_FFFFFF,t_70#pic_center" alt></p><ul><li><p>我们找到最后一行，无穷样本在α为0.05的值为1.960。</p></li><li><p>最后作比较：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_76.png" alt></p><blockquote><p>补充说明：如果此时求出的值大与误差值，代表我们取样的假设不可以被接受，如果小于则可以被接受。</p></blockquote></li></ul><h1 id="第九题"><a href="#第九题" class="headerlink" title="第九题"></a>第九题</h1><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_9.png" alt></p><blockquote><p>本次我们将会使用单因素方差分析，本质上就是填表和计算公式，跟着下面的步骤一个一个变量的对照计算将会得到答案。</p></blockquote><ul><li><p>首先我们来看题目所给的数据结构一般格式：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_91.png" alt></p></li></ul><blockquote><p>在本题中，行业因素，每一个行业就代表水平，下面的值代表观测值。</p></blockquote><ul><li><p>首先我们要提出假设，基本格式为：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_92.png" alt></p></li></ul><blockquote><p>因为本题中有四个行业，所以我们有四个μ。</p></blockquote><p>​        <img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_93.png" alt></p><ul><li><p>接下来一步，我们需要根据所给数据填表，首先来看一下表的格式：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_94.png" alt></p></li></ul><p><strong>本题中：k是4，n是26，什么意思自己看一下题目，不再赘述</strong></p><blockquote><p>其中涉及到一些公式，我们列到下边：</p><ul><li><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_95.png" alt></li><li><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_96.png" alt></li><li><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_97.png" alt></li></ul></blockquote><ul><li><p>题目中已经给出部分数据，所以我们直接填表</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_98.png" alt></p></li><li><p>最后判断因素的水平对其观测值是否有显著影响，也就是比较组间方差与组内方差之间的差异大小，将检验统计量F的值与给定的α下的临界值Fα进行比较，就可以作出对原假设H0的决策。</p><p>若F&gt;Fα，则拒绝原假设，因素水平对观测值有显著影响；</p><p>若F&lt;Fα，则接受原假设，不能认为因素水平对观测值有显著影响。</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_99.png" alt></p></li><li><p>而我们的临界值依旧是查表：</p><p><img src="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fpic.chinawenben.com%2Fupload%2F1_28355q5vr7883j8vaq7xb5do.jpg&amp;refer=http%3A%2F%2Fpic.chinawenben.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1641435593&amp;t=b9a988c7268224d7a01251c1c455b892" alt></p></li></ul><blockquote><p>找到22的行，第3列即是3.05。</p></blockquote><h1 id="第十题"><a href="#第十题" class="headerlink" title="第十题"></a>第十题</h1><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_10.png" alt></p><ul><li><p>首先我们来看一下无偏估计量的定义：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_101.png" alt></p></li><li><p>本题要是硬要分析的话就是，我们可以把1提出来，然后因为每个样本就是独立的，所以每一个E都为μ，然后得：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_102.png" alt></p></li></ul><h1 id="第十一题"><a href="#第十一题" class="headerlink" title="第十一题"></a>第十一题</h1><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t11.png" alt></p><ul><li><p>首先要解释一下他们这些样本都是独立的，然后因为是独立的就可拆开来搞来搞去了，也没什么好分析的：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_111.png" alt></p></li><li><p>最后看一下哪个D最小，就可以判定哪个最有效了：</p><p><img src="/2021/12/06/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E6%9C%9F%E6%9C%AB%E8%80%83%E8%AF%95%E9%A2%98%E7%9B%AE%E5%88%86%E6%9E%90/t_112.png" alt></p></li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>数理统计就是公式比较多，概念也不少，但是都是由一些基本的知识点组成的。还有就是需要会查表，但是不用担心，考试的时候会把需要用到的数据给你。最后就是还是要自己把答案挡住看看能不能理清自己的逻辑解题。</p>]]></content>
      
      
      <categories>
          
          <category> 期末考试复习资料 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数理统计 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习吴恩达-学习笔记-第九章</title>
      <link href="/2021/12/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B9%9D%E7%AB%A0/"/>
      <url>/2021/12/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B9%9D%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络的学习过程"><a href="#神经网络的学习过程" class="headerlink" title="神经网络的学习过程"></a>神经网络的学习过程</h1><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>神经网络分类问题有两种：</p><ul><li>二分类问题：<ul><li>只有一个输出单元</li></ul></li><li>多分类问题：<ul><li></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习吴恩达-学习笔记-第八章</title>
      <link href="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/"/>
      <url>/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h2><p>当我们使用线性模型拟合非线性的数据集时，特征量通常会很多，如果再构造高阶多项式，我们的回归模型复杂度将会非常高。</p><p><strong>例子</strong></p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_1.png" alt="例图"></p><p>假设我们仅使用50*50的像素灰图片，选择每个像素点为特征，则特征总量为N=2500,如果选用RGB则N=7500，如果将两个组合为新特征，则特征数为30，000，000个</p><h2 id="神经元和大脑"><a href="#神经元和大脑" class="headerlink" title="神经元和大脑"></a>神经元和大脑</h2><p>神经网络就源于模拟人类大脑，但需要计算的量很大。随着计算机硬件性能的提高，神经网路又重新变为主流机器学习应用算法。</p><p>我们希望开发一种算法，可以对传进来的任何数据进行处理学习。</p><h2 id="模型展示"><a href="#模型展示" class="headerlink" title="模型展示"></a>模型展示</h2><p>我们一般把神经网络分为三部分，即输入层，隐藏层，输出层。</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_2.png" alt="模型展示"></p><p>图中输入层对应输入单元，隐藏层对应中间单元（但不一定是一层），输出层对应的是输出单元。中间单元应用激活函数处理数据。</p><p>我们定义：</p><blockquote><p>$x_0$:偏置单元，$x_0=1$</p><p>$\theta$:参数，权重</p><p>激活函数：g，即逻辑函数等</p><p>输入层：对应训练集中的特征x</p><p>输出层：对应于训练集的结果y</p><p>$ a_i^{(j)} $:第j层的第i个激活单元</p><p>$\theta^{(j)}$:从第j层映射到第j+1层的权重矩阵</p><p>$\theta^{(j)}_{v,u}$:从第j层第u个单元映射到第j+1层的第v个单元的权重</p><p>$s_j$:第j层的激活单元数目</p></blockquote><p><strong>向前传播</strong>：</p><p>对输入层的所有激活单元应用激活函数，得到隐藏层的输入，对于接下来的每一层都通过激活函数，一层一层的向下计算传递结果。</p><p><strong>向量化实现</strong>：</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_2.png" alt="向量化实现"></p><p>更一般的：</p><p>$z<em>i^{(j)}=\theta</em>{i,0}^{(j-1)}a<em>0^{(j-1)}+\theta</em>{i,1}^{(j-1)}a<em>1^{(j-1)}+…+\theta</em>{i,n}^{(j-1)}a_n^{(j-1)}$</p><p>$z^{(j)}=\theta^{j-1}a^{j-1}$</p><p>$a^{(j)}=g(z^{(j)})$</p><p>扩展到所有样本实例：</p><p>$z^{(2)}=\theta^{(1)}X^T$</p><p>这时是一个$ s_2*m $的矩阵</p><blockquote><p>m:训练集中的样本实例数目</p><p>$s_2$:第二层神经网络中激活单元的数目</p></blockquote><p>神经网络也可以有多层，每层激活单元的数目是不固定的：</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_3.png" alt="多层神经网络"></p><h2 id="例子直观理解"><a href="#例子直观理解" class="headerlink" title="例子直观理解"></a>例子直观理解</h2><p><strong>例1</strong></p><p>为了更好的演示神经网络的工作过程，以单层网络进行逻辑运算的拟合作为例子。</p><p>下面的例子中$x_1x_2$为二进制数</p><p>逻辑与运算神经网络：</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_4.png" alt="神经网络例子"></p><p>$ \theta^{(1)}=[-30，20，20] ，h_\theta(x)=g(-30+20x_1+20x_2)$</p><p>回归激活函数图像，对应图中表格，就实现了一个能够进行与运算的神经网络。</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_5.png" alt="激活函数"></p><p><strong>例2</strong></p><p>下面将构建一个更为复杂的神经网络</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_6.png" alt="神经网络例子"></p><p>我们通过拟合逻辑与，或，非的运算得到了三个单层简单的运算。</p><p>下面我们将三个简单的单层神经网络组合进行更为复杂的神经网络运算。</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_7.png" alt="神经网络例子"></p><blockquote><p>在隐藏层中我们添加了一个偏置项，控制最后的输出。</p></blockquote><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>在很多实际问题中，我们将会面临很多复杂的问题，当我们需要进行多分类问题时，我们只需要把最终的输出层以及输入时的标签都改成四维向量进行拟合模型就可以了。</p><p><img src="/2021/11/27/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AB%E7%AB%A0/ch8_8.png" alt="多分类问题"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习吴恩达-学习笔记-第七章</title>
      <link href="/2021/11/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E7%AB%A0/"/>
      <url>/2021/11/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h2><p>拟合效果的三种情况：</p><ul><li><p>欠拟合</p><p>没有很好的拟合实际的数据集，导致预测的效果不好。</p></li><li><p>合理拟合</p><p>很好的拟合程度，能给出不错的结果。</p></li><li><p>过拟合</p></li></ul><p>​        过分的考虑了数据集所包含的信息，使得拟合程度很高，泛化程度很低，拟合模型过于复杂时会导致这种情况的发生。</p><p><img src="/2021/11/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E7%AB%A0/ch7_1.png" alt="过拟合图"></p><p> 解决过拟合的两个办法：</p><ul><li>尽量选择少的特征进行训练模型<ul><li>缺点是会舍弃部分特征</li></ul></li><li>正则化：考虑每一个特征对最终预测的影响，进行适当的调整。</li></ul><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><p>很多时候，由于特征数目很多我们并不知道预先要保留哪些特征，我们就可以应用正则化来进行对特征处理。</p><p>在某些实际问题中，我们最终拟合的假设函数可能是一些带有高阶参数的函数，例如，$ \theta_0 + \theta_1x+ \theta_1x^2+ \theta_3x^3+\theta_4x^4$,正是由于高次未知数的存在使得我们最终的曲线很复杂导致了过拟合的结果。</p><p>我们可在不减小特征数目的情况下，如果能消除高次未知数的影响，最终的函数就能变得简单。</p><p>为了保留各部分的参数，我们不修改假设函数，我们可以修改代价函数来控制高次未知数：</p><script type="math/tex; mode=display">\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 + 1000*\theta_3^2 + 1000*\theta_4^2</script><p>上式中，随着我们对代价函数进行最小化，我们同时也对高次未知数做了最小化处理，最终会使得假设函数变得很简单。</p><p><img src="/2021/11/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E7%AB%A0/ch7_2.png" alt="代价函数图"></p><p>上边我们提到，将会选择参数进行缩小，最终使得假设函数尽可能地简单来避免过拟合的问题，但是由于实际问题中我们预先并不知道该选择什么参数进行缩小，所以我们缩小所有参数来一定程度上避免过拟合问题。</p><p>我们提出下面的公式来描述正则化：</p><script type="math/tex; mode=display">\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]</script><p>其中λ是正则化参数，类似于学习效率α也需要我们进行选择</p><ul><li>λ过大时<ul><li>无法有效的拟合数学模型</li><li>梯度下降可能无法收敛</li></ul></li><li>λ过小时<ul><li>无法有效解决过拟合问题</li></ul></li></ul><blockquote><p>奥卡姆剃刀原理：“切勿浪费较多东西去做，用较少的东西，同样可以做好的事情。”</p><p>正则化便符合奥卡姆剃刀原理，是我们解决过拟合一个行之有效的手段，我们可以认为复杂的模型具有较大的先验概率，简单的模型有较小的先验概率。</p></blockquote><h2 id="线性回归正则化"><a href="#线性回归正则化" class="headerlink" title="线性回归正则化"></a>线性回归正则化</h2><p><img src="/2021/11/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E7%AB%A0/ch7_3.png" alt="线性回归正则化"></p><p>上图中上半部分就是，我们为线性回归代价函数加入了正则化项的梯度下降算法的式子。</p><blockquote><p>注意：我们不对$ \theta_0 $做任何的惩罚。</p></blockquote><p>下半部分是我们添加了正则化项的代价函数后的另一种表达形式，前半部分的式子在进行梯度下降后会得到一个系数，它总是小于1，我们可以直观地认为它缩小了正在优化的参数。</p><h2 id="逻辑回归正则化"><a href="#逻辑回归正则化" class="headerlink" title="逻辑回归正则化"></a>逻辑回归正则化</h2><p><img src="/2021/11/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%83%E7%AB%A0/ch7_4.png" alt="逻辑回归正则化"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习吴恩达-学习笔记-第六章</title>
      <link href="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/"/>
      <url>/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><blockquote><p>虽然逻辑回归名字里有回归，但是是实际上是一个描述分类的算法。</p></blockquote><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><p>邮件问题：是不是垃圾邮件？</p><p>肿瘤问题：是不是恶性肿瘤？</p><script type="math/tex; mode=display">y\in(0,1)</script><p>0：通常表示负类。</p><p>1：通常表示正类。</p><blockquote><p>多分类问题是二分类问题的扩展。  </p></blockquote><p><strong>样例</strong>：</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_1.png" alt="引出逻辑回归问题图"></p><p>图中是一个二分类问题，我们的目标是预测最终的离散结果。</p><blockquote><p>在分类问题中，我们通常会设置一个阈值，来对最终的$h(x)$的输出结果进行判断。</p></blockquote><p>本样例中假设我们选择0.5作为阈值，即：</p><script type="math/tex; mode=display">h(x)\geq 0.5 $$则我们预测$y=1$,即正类；$$ h(x)< 0.5 $$则我们预测$y=0$,即负类。当我们使用线性回归拟合数据时，我们发现:- 加入了偏差项，即右侧点后线性回归所拟合的直线并不能很好的对数据集进行分类。- 线性回归的输出为实数集，当线性回归给出很大或者很小的数值时$y\notin(0,1)$。和线性回归不同逻辑回归是一个分类算法，其输出值在(0,1)范围内，可以更好的解决问题## 假设函数表示我们的目标：$0\leq h_\theta(x)\leq1$原本的函数：$ h_\theta(x) = \theta^Tx$> 通过将$ h_\theta(x)$的输出作为另一个函数$ g(x) $的输入，最终得到一个$ \in(0,1)$的输出，用于分类。定义的激活函数：$ g(z) = \frac{1}{1+e^-z} $当我们把原本的函数带入激活函数，我们将得到：$$ g(z) = \frac{1}{1+e^{-\theta^Tx}}</script><p>激活函数的图像：</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_2.png" alt="激活函数图像"></p><p>关于激活函数的说明：</p><ul><li>因为我们最终得到的是介于(0,1)之间的数值，其代表的是某样本最终结果是正例的概率。</li></ul><p><strong>例子</strong>:</p><p>如果$ x = \left[ \begin{matrix} x<em>0\x_1\…\x_n \end{matrix} \right]$得到的$ h</em>\theta = 0.7 $表示有70%的概率此样例的最终预测值为正例。</p><h2 id="决策边界"><a href="#决策边界" class="headerlink" title="决策边界"></a>决策边界</h2><blockquote><p>决策边界可以让我们更好的理解逻辑回归的原理</p></blockquote><ul><li><p>由上述定义，我们知道当$ h<em>\theta(x) $&gt;0时，我们便预测为正例。$ h</em>\theta(x) $&lt;0时我们便预测为负例。</p></li><li><p>由激活函数的处理以及我们设定的阈值0.5，我们知道：</p></li></ul><p>$ g(z) &lt;0.5$时我们预测为0，反之则为1。</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_2.png" alt="决策边界图"></p><blockquote><p>注：$z = h_\theta(x)=\theta^Tx$</p><p>如图所示，当z的值非常大时g(z)将会趋向于无穷；当z的值非常小时g(z)将会趋近于0。</p></blockquote><p><strong>例子</strong>：</p><p>设$ h_\theta(x)= g(\theta_0+\theta_1x_1+\theta_1x_2)$是我们的假设函数</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_3.png" alt="决策边界例图"></p><p>假设我们拟合的参数为：$ \theta = \left[ \begin{matrix} -3\1\1 \end{matrix} \right] $,则有$ z = -3 + x<em>1 + x_2$,根据如上定义当$ h</em>\theta(x) $&gt;0时，我们便预测为正例。$ h_\theta(x) $&lt;0时我们便预测为负例。决策边界即为$ x_1+x_2 = 3 $,当我们画出该直线时我么可以看出其将正负例很好的分割了。</p><p>在本例中$ x_1+x_2 = 3 $就为决策边界。</p><blockquote><p>注意：在实际解决问题的过程中，由于我们需要拟合很多不同的数据集，而且我们也有可能遇到需要进行合并特征的情况，我们也会使用多项式拟合数据，会得到很多不同的决策边界，例如：圆，椭圆等。</p></blockquote><p><strong>例图</strong>：</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_4.png" alt="决策边界例图"></p><h2 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h2><blockquote><p>为了确定我们在逻辑回归中的参数，我们在逻辑回归中也需要一个代价函数来进行参数的选择。</p></blockquote><p>在以前的问题中，我们为线性回归定义了线性回归的代价函数：</p><p>$ J(\theta) = \frac{1}{2m}\sum<em>{i=1}^{m}(h</em>\theta(x^{(i)})-y^{(i)})^2 $</p><p>但是在逻辑回归中使用此函数将使得$J(\theta)$的图像，如下图</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_5.png" alt="代价函数图"></p><p>图中情况很难使用一种行之有效的方式进行最小化代价函数，所以我们选择另一种代价函数。</p><p><strong>对数损失函数</strong>：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)})</script><p>$ Cost(h<em>\theta(x^{(i)}),y^{(i)}) = -log(h</em>\theta(x))     $           if  y=1</p><p>$ Cost(h<em>\theta(x^{(i)}),y^{(i)}) = -log(1-h</em>\theta(x))$    if  y=0</p><p><strong>对数损失函数的原理</strong>：</p><blockquote><p>注意：上文我们提到通过对本来的输出值做激活函数的处理所以我们最终的输出值∈(0,1)</p></blockquote><p>对数损失函数的图像如下所示：</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_6.png" alt="对数损失函数"></p><p>在上图中，我们看出当实际的y值为1时，h(x)的输出结果约靠近1，J（θ）的值越小，即惩罚越小。</p><p><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_7.png" alt="对数损失函数"></p><p>在上图中，我们看出当实际的y值为0时，h(x)的输出结果约靠近0，J（θ）的值越小，即惩罚越小。</p><p>所以我们最终的目标依然是，找到一种方法确定θ使得损失函数尽量的减小。</p><h2 id="代价函数地简写形式"><a href="#代价函数地简写形式" class="headerlink" title="代价函数地简写形式"></a>代价函数地简写形式</h2><p>为让代价函数简写成一个式子，我们提出一种简写代价函数的方法：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x^{(i)}),y^{(i)}) = -\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}log(h_\theta(x)+(1-y^{(i)})log(1-(h_\theta(x))]</script><blockquote><p>当y为1时，括号内第二项为零，当y为0时，括号内第一项为0。</p></blockquote><p>在逻辑回归中，我们亦可以采用之前线性回归的梯度下降算法进行参数优化，不同点只是梯度下降中的$ h_\theta(x) $发生了变化。</p><h2 id="高级优化"><a href="#高级优化" class="headerlink" title="高级优化"></a>高级优化</h2><p>除了梯度下降外，还有一些更优的最小化代价函数的方法，我们只需要计算出代价函数和偏导数传入算法中，我们就能获得参数的最优解。这类算法被称为<strong>最优化算法</strong>：</p><p>最优化算法的例子：</p><ul><li>梯度下降算法</li><li>共轭梯度算法</li><li>牛顿法和拟牛顿法<ul><li>DFP算法</li><li>局部优化算法（BFGS）</li><li>有限内存局部优化法（L_BFGS）</li></ul></li><li>拉格朗日数乘法</li></ul><p>最优算法将会很复杂，但是不需要我们选择学习速率α，我们在使用时又是并不需要完全理解内部逻辑，有很多代码库对诸多算法进行了封装供我们使用。</p><h2 id="多分类问题"><a href="#多分类问题" class="headerlink" title="多分类问题"></a>多分类问题</h2><p>多分类问题例子：</p><p>对天气进行预测，预测晴天还是阴天或者下雨。</p><p>解决多分类问题的一个方法就是把它认为是多个二分类问题，称为<strong>One-vs-all</strong>：<br><img src="/2021/11/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%85%AD%E7%AB%A0/ch6_8.png" alt="多分类问题图"></p><p>如图所示，我们为每一个类别都进行逻辑回归，得到三个模型，每个模型值判断是否为第i类。</p><blockquote><p>k表示一共有k类需要我们进行逻辑回归。</p></blockquote><p>在多分类问题中，我们输出的结果不再是一个数值，而是一个k阶向量。最终输出的向量值代表了是第i类的可能性。</p><p>最终我们观察哪一个类别的可能性最高，然后作为我们最终的预测值。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习吴恩达-学习笔记-第四章</title>
      <link href="/2021/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/"/>
      <url>/2021/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h1><h2 id="多特征"><a href="#多特征" class="headerlink" title="多特征"></a>多特征</h2><ul><li>在解决实际问题的时候，通常会有很多描述不同性质的特征。例如在房屋预测问题上，房屋的年限，卧室的个数等。</li><li><p>多变量假设函数h表示为：$ h_\theta(x) = \theta_0 + \theta_1x_1+…+\theta_nx_n $</p></li><li><p>为了表示方便我们为$ \theta_0 $也添加一个变量$ x_0 $并且使其一直为1，那么多特征的线性回归方程可y引入线性代数表示为：</p></li></ul><script type="math/tex; mode=display">h_\theta(x) =[\theta_0,\theta_1...\theta_n] \left[ \begin{matrix} x_0\\x_1\\...\\x_n \end{matrix} \right]=\theta^Tx</script><blockquote><p>n:特征总数</p><p>$ x^{(i)} $:代表样本矩阵中第i行，也就是第i个训练实例。</p><p>$ x^{(i)}_{j} $:代表样本矩阵第i行第j列，也就是第i个实例的第j个向量的值。</p></blockquote><h2 id="多变量梯度下降"><a href="#多变量梯度下降" class="headerlink" title="多变量梯度下降"></a>多变量梯度下降</h2><blockquote><p>多变量梯度下降和单变量梯度下降类似，在多变量梯度下降中，我们的目标就是不断更新n个θ的值。</p></blockquote><p><strong>多变量梯度下降展开式：</strong></p><p><img src="/2021/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/ch4_1.png" alt="多变量梯度下降公式"></p><h2 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h2><ul><li><p>进行梯度下降时，特征值的取值范围有可能会有不同，这会影响代价函数收敛的速度。</p></li><li><p>为了优化梯度下降的速率我们将会使用<strong>特征缩放</strong>，使得各特征取值尽量一致。</p></li><li><p>均值归一化：</p><p>$ x_i:=\frac{x_i-average(x)}{max(x)-min(x)} $</p></li></ul><h2 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h2><p>首先介绍两种确定函数是否收敛的方法以及特点：</p><ul><li><p>多次迭代收敛法：通过绘制代价函数关于迭代次数的图像，可视化梯度下降的过程。</p><p><img src="/2021/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/ch4_2.png" alt="学习速率图"></p></li></ul><blockquote><p>横轴表示运行梯度下降的次数，纵轴表示运行相对于横轴次数的 J(θ) ，我们可以观察其曲线的状态来判断是否收敛，以及迭代次数和α的取值。</p></blockquote><p><img src="/2021/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/ch4_3.png" alt="学习速率图"></p><blockquote><p>当图像波动时，证明选择的速率过大，导致工作不正常。</p><p>当图像很平稳时，证明选择的速率过小，导致梯度下降缓慢。</p></blockquote><ul><li>特点：<ul><li>无法确定需要多少次迭代</li><li>较容易绘制关于迭代次数的图像</li><li>根据图像易预测所需要的迭代次数</li></ul></li><li>自动化检测收敛法：选择一个阈值，当J(θ)小于这个值的时候就判定已经收敛</li><li>特点：<ul><li>不易选择阈值</li><li>代价函数接近直线时无法确定收敛情况</li></ul></li></ul><h2 id="特征和多项式回归"><a href="#特征和多项式回归" class="headerlink" title="特征和多项式回归"></a>特征和多项式回归</h2><ul><li><p>在选取特征时，有时我们可以进行人为合并来为训练模型提供便利。</p></li><li><p>线性回归只能以直线进行拟合数据，有时也需要曲线来拟合数据，即多项式回归。</p><p><img src="/2021/11/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E5%9B%9B%E7%AB%A0/ch4_4.png" alt="多项式回归图"></p><p><strong>注意</strong>：进行多项式回归一定要进行特征缩放，因为有指数的存在所以自变量的范围会进一步扩大。</p></li></ul><h2 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h2><p>对于<strong>线性方程</strong>还有一种方法来解决问题就是<strong>正规方程</strong>。</p><p>正规方程法，即令偏导数为0，通过解析函数的方式直接计算出参数向量的值。</p><blockquote><p>本质上就是根据当导数的值为0时，函数获得最小值。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>西瓜书-第三章-学习笔记</title>
      <link href="/2021/11/22/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/11/22/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>笔记的前一部分主要是对机器学习预备知识的概括，包括机器学习的定义/术语、学习器性能的评估/度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容—线性模型。</p><h1 id="3、线性模型"><a href="#3、线性模型" class="headerlink" title="3、线性模型"></a><strong>3、线性模型</strong></h1><p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p><h2 id="3-1-线性回归"><a href="#3-1-线性回归" class="headerlink" title="3.1 线性回归"></a><strong>3.1 线性回归</strong></h2><p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000—&gt;13亿…2016—&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）—&gt;15k。</p><p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p><ul><li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p></li><li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p></li></ul><p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p><p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y=wx+b需要写成：</p><p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p><p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p><p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p><p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p><p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p><h2 id="3-2-线性几率回归"><a href="#3-2-线性几率回归" class="headerlink" title="3.2 线性几率回归"></a><strong>3.2 线性几率回归</strong></h2><p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p><p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p><p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p><h2 id="3-3-线性判别分析"><a href="#3-3-线性判别分析" class="headerlink" title="3.3 线性判别分析"></a><strong>3.3 线性判别分析</strong></h2><p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p><p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p><ul><li>类内散度矩阵（within-class scatter matrix）</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p><ul><li>类间散度矩阵(between-class scaltter matrix)</li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p><p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p><p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p><p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p><p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。<br>​             </p><h2 id="3-4-多分类学习"><a href="#3-4-多分类学习" class="headerlink" title="3.4 多分类学习"></a><strong>3.4 多分类学习</strong></h2><p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p><ul><li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p></li><li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p></li><li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。</p></li></ul><p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p><h2 id="3-5-类别不平衡问题"><a href="#3-5-类别不平衡问题" class="headerlink" title="3.5 类别不平衡问题"></a><strong>3.5 类别不平衡问题</strong></h2><p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p><ol><li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li><li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li><li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li></ol><p>​<br>​      </p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 西瓜书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习吴恩达-学习笔记-第二章</title>
      <link href="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/"/>
      <url>/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h1><h2 id="房价预测（典型的监督学习中回归问题的例子）"><a href="#房价预测（典型的监督学习中回归问题的例子）" class="headerlink" title="房价预测（典型的监督学习中回归问题的例子）"></a>房价预测（典型的监督学习中回归问题的例子）</h2><ul><li>横轴为不同的房屋面积，纵轴是房屋的出售价格。</li></ul><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_1.png" alt="房价回归图"></p><h2 id="监督学习的基本工作模式"><a href="#监督学习的基本工作模式" class="headerlink" title="监督学习的基本工作模式"></a><strong>监督学习的基本工作模式</strong></h2><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_2.png" alt="监督学习基本工作模式"></p><ul><li>把训练集中的数据交给学习算法。</li><li>学习算法拟合训练集中的数据并输出一个函数用h表示。</li><li>h根据输入的x输出对应的预测y值，可以说h是x到y的一个映射。</li></ul><h2 id="单变量线性回归表达式"><a href="#单变量线性回归表达式" class="headerlink" title="单变量线性回归表达式"></a>单变量线性回归表达式</h2><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_3.png" alt="单变量线性回归表达式"></p><h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><blockquote><p>损失函数：计算单个样本的误差。<br>代价函数：计算整个训练集所有损失函数之和的平均值</p></blockquote><ul><li>我们要根据数据集拟合一个函数求解一个预测结果h最接近实际结果y时参数的取值，则可以表达为如下式子的最小值：</li></ul><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_4.png" alt="代价函数式子图"></p><ul><li>图中蓝色部分就是我们最终预测和实际结果的误差。</li></ul><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_5.png" alt="误差结果例图"></p><p><strong>根据上部分我们可以定义：</strong></p><ul><li><p>假设函数：$ h_\theta(x) = \theta_0 + \theta_1x. $</p></li><li><p>参数：$ \theta_0,\theta_1 $</p></li><li><p>代价函数：$ J(\theta<em>0,\theta_1) = \frac{1}{2m}\sum</em>{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 $</p></li><li><p>目标：最小化 $ J(\theta_0,\theta_1)$</p><p>图中将展示代价函数的工作原理：</p><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_6.png" alt="代价函数工作原理"></p><p>右图中随着$ \theta $的变化，$ J(\theta_0,\theta_1) $取得了最小值，相对于左侧函数来说就是h拟合的最好的情况。</p><blockquote><p>当参数很多时，代价函数的工作过程讲在多维上进行，此处略。</p></blockquote><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_7.png" alt="二维代价函数图"></p></li></ul><h1 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h1><p>在特征数很多的时候，我们需要一种方法来让计算机自动化找出最好拟合数据的函数h,换句话说也就是取到代价函数最小值时的$  \theta$，即<strong>梯度下降算法</strong>。</p><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_8.png" alt="梯度下降原理图"></p><p>梯度下降的原理：</p><ul><li>开始时随计选择一个参数组合，计算代价函数，寻找下一组能使大家函数下降的最多的组合，最后便能找到一个最小值，但此时的最小值只是<strong>局部最小值</strong>。</li><li>我们还需要选择不同的初始参数组合做对比来做相对比较以用来找到相对最小值。我们并不能知道当前的最小值是不是<strong>全局最小值</strong>。</li></ul><p>梯度下降的公式:</p><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_9.png" alt="梯度下降公式"></p><blockquote><p>本质上可能就是不断地求θ的偏导数，然后不断地下降。</p></blockquote><ul><li>聚焦公式，其中的α表示了梯度下降的步长，决定每步走多少距离。</li><li>公式中的偏导数才是决定下降的方向的关键。</li></ul><p><strong>注意</strong>：在计算时要进行批量更新而不是分开更新，否则结果上会有很大的不确定。</p><h2 id="梯度下降的补充理解"><a href="#梯度下降的补充理解" class="headerlink" title="梯度下降的补充理解"></a>梯度下降的补充理解</h2><p>上部分提到α和偏导数分别代表步长和方向，下面做补充解释：</p><ul><li><p>为简化问题便于理解，此处只考虑单变量的梯度下降，我们此处考虑一个变量参数θ，梯度下降中θ的更新公式为：<br><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_10.png" alt="梯度下降公式"></p><blockquote><p>由于只考虑一个变量θ，此处的导数计算可以用d表示。</p></blockquote></li><li><p>如果当前θ的映射出的 J(θ)在最小值的右侧，由于导数的值又表示当前点的切线的斜率，所以此时求得的公式中的值为正，当减去了正值之后θ在图中的位置将会向左移动,反之公式中的值为负，当减去了负值之后θ在图中的位置将会向右移动，最终收敛到最小值。</p></li></ul><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_11.png" alt="梯度下降原理"></p><blockquote><p>图中只考虑了二维的情况，多维的情况原理也相同。</p></blockquote><ul><li><p>下面关注步长α的意义，对于α来讲我们需要一个合适的值才能使得梯度下降运行良好。</p></li><li><p>α过小算法下降效率将会很低下，需要很多步骤才能完成最终的梯度下降，需要很高的时间复杂度，如图所示：</p><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_12.png" alt="梯度下降原理"></p></li><li><p>α过大算法将会运行不正常，有可能在迭代的过程中最小值直接被跳过，如图所示：</p><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%BA%8C%E7%AB%A0/ch2_13.png" alt="梯度下降原理"></p></li></ul><h2 id="线性回归中的梯度下降"><a href="#线性回归中的梯度下降" class="headerlink" title="线性回归中的梯度下降"></a>线性回归中的梯度下降</h2><p>线性回归模型</p><ul><li><p>$ h_\theta(x) = \theta_0 + \theta_1x. $</p></li><li><p>$ J(\theta<em>0,\theta_1) = \frac{1}{2m}\sum</em>{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2 $</p></li></ul><blockquote><p> 在线性回归中，梯度下降只要把h带入就完成了，此处不再展示。</p></blockquote><p>由于线性函数的回归函数只有一个全局的最优解，线性回归函数求解最小值问题属于<strong>凸函数优化问题</strong>。</p><p>第三章部分主要是线性代数的复习，不再做记录。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习吴恩达-学习笔记-第一章</title>
      <link href="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/"/>
      <url>/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><h2 id="什么是机器学习"><a href="#什么是机器学习" class="headerlink" title="什么是机器学习"></a>什么是机器学习</h2><ul><li>在没有明确设置的情况下使得计算机具有学习能力的领域。 -Samuel(1959)</li><li>计算机程序从经验E中学习解决某一任务T进行某个度量P通过测量P在T上的表现因经验E而提高。 -Tom (1998)<h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2></li><li>监督学习：在监督学习中，我们想要计算机预测“正确值”。<ul><li>回归问题：预测一组连续的值。</li><li>分类问题：预测离散的值。<h3 id="监督学习的例子"><a href="#监督学习的例子" class="headerlink" title="监督学习的例子"></a>监督学习的例子</h3></li></ul></li><li><p>利用监督学习预测房价（回归问题）</p><ul><li>大多数情况下会拟合一条直线来进行回归</li><li><p>但是有时候会选择不一样的曲线进行拟合效果更好</p><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/ch1_1.jpg" alt="回归问题例图"></p></li></ul></li><li><p>利用监督学习推测乳腺癌良性与否（分类问题）</p><ul><li><p>尝试推测出离散的结果</p><p><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/ch1_2.png" alt="分类问题例图"></p><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2></li></ul></li><li>根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。<ul><li>无监督学习里典型例子是聚类。聚类的目的在于把相似的东西聚在一起，而我们并不关心这一类是什么。<h3 id="无监督学习的例子"><a href="#无监督学习的例子" class="headerlink" title="无监督学习的例子"></a>无监督学习的例子</h3></li></ul></li><li>聚类问题<br><img src="/2021/11/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%B4%E6%81%A9%E8%BE%BE-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC%E4%B8%80%E7%AB%A0/ch1_3.png" alt="聚类问题例图"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>西瓜书-第二章-学习笔记</title>
      <link href="/2021/11/22/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/11/22/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>本篇主要是对第二章剩余知识的理解，包括：性能度量、比较检验和偏差与方差。在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练/测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。</p><p><strong>2.5 性能度量</strong></p><p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p><p><strong>2.5.1 最常见的性能度量</strong></p><p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf76276.png" alt="1.png"></p><p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度=1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4c704.png" alt="2.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf6fb84.png" alt="3.png"></p><p><strong>2.5.2 查准率/查全率/F1</strong></p><p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准/查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准/查全率定义如下：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf885a4.png" alt="4.png"></p><p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf871a6.png" alt="5.png"></p><p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p><p>“P-R曲线”正是描述查准/查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71dafc4411.png" alt="6.png"></p><p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。</p><p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf40ff6.png" alt="7.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf75407.png" alt="8.png"></p><p>特别地，当β=1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p><p><img src="https://i.loli.net/2018/10/17/5bc71daf20885.png" alt="9.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71daf4b90a.png" alt="10.png"></p><p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70230e.png" alt="11.png"></p><p><strong>2.5.3 ROC与AUC</strong></p><p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6bee91.png" alt="12.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed75cefe.png" alt="13.png"></p><p>简单分析图像，可以得知：当FN=0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p><p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed740a24.png" alt="14.png"></p><p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p><p><strong>2.5.4 代价敏感错误率与代价曲线</strong></p><p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病—&gt;有疾病只是增多了检查，但有疾病—&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p><p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p><p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p><p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p><p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p><p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 西瓜书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>西瓜书-第一章-学习笔记</title>
      <link href="/2021/11/20/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/11/20/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h1><p>傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！哈哈，也希望自己这学期的machine learning课程取得一个好成绩！</p><h2 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h2><p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p><p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p><ul><li>P：计算机程序在某任务类T上的性能。</li><li>T：计算机程序希望实现的任务类。</li><li>E：表示经验，即历史的数据集。</li></ul><p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p><h2 id="机器学习的一些基本术语"><a href="#机器学习的一些基本术语" class="headerlink" title="机器学习的一些基本术语"></a>机器学习的一些基本术语</h2><p>假设我们收集了一批西瓜的数据，例如：（色泽=青绿;根蒂=蜷缩;敲声=浊响)， (色泽=乌黑;根蒂=稍蜷;敲声=沉闷)， (色泽=浅自;根蒂=硬挺;敲声=清脆)……每对括号内是一个西瓜的记录，定义：     </p><ul><li>所有记录的集合为：数据集。</li><li>每一条记录为：一个实例（instance）或样本（sample）。</li><li>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</li><li>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</li><li><p>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p><p> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：    </p></li><li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p></li><li>所有测试样本的集合为：测试集（test set），[一般]。  </li><li><p>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p><p> 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：    </p></li><li><p>预测值为离散值的问题为：分类（classification）。</p></li><li><p>预测值为连续值的问题为：回归（regression）。</p><p> 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：    </p></li><li><p>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p></li><li>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</li></ul><h1 id="模型的评估与选择"><a href="#模型的评估与选择" class="headerlink" title="模型的评估与选择"></a>模型的评估与选择</h1><h2 id="误差与过拟合"><a href="#误差与过拟合" class="headerlink" title="误差与过拟合"></a>误差与过拟合</h2><p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：    </p><ul><li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li><li>在测试集上的误差称为测试误差（test error）。</li><li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li></ul><p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p><ul><li>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</li><li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li></ul><p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p><p><img src="https://i.loli.net/2018/10/17/5bc7181172996.png" alt></p><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h2><p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p><p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p><p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p><h2 id="训练集与测试集的划分方法"><a href="#训练集与测试集的划分方法" class="headerlink" title="训练集与测试集的划分方法"></a>训练集与测试集的划分方法</h2><p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p><h3 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h3><p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D=S∪T且S∩T=∅，常见的划分为：大约2/3-4/5的样本用作训练，剩下的用作测试。需要注意的是：训练/测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p><h3 id="交叉验证法"><a href="#交叉验证法" class="headerlink" title="交叉验证法"></a>交叉验证法</h3><p>将数据集D划分为k个大小相同的互斥子集，满足D=D1∪D2∪…∪Dk，Di∩Dj=∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集/测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p><p><img src="https://i.loli.net/2018/10/17/5bc718115d224.png" alt></p><p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练/测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p><h3 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h3><p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p><p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p><p><img src="https://i.loli.net/2018/10/17/5bc71811246dd.png" alt></p><p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集/测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p><h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p><p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练/测试集就有5<em>5</em>5= 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p><p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 西瓜书 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习实战-第二章-学习笔记</title>
      <link href="/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><ul><li><a href="https://github.com/ageron/handson-ml2">获取数据集和源码</a><blockquote><p>作者建议使用脚本获取即时的数据，但是由于本数据是静态的所以暂且不使用作者书写脚本的方法。</p></blockquote></li></ul><h1 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h1><h2 id="编写加载数据的函数"><a href="#编写加载数据的函数" class="headerlink" title="编写加载数据的函数"></a>编写加载数据的函数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATA_PATH = &quot;datasets\housing&quot;</span><br><span class="line">def load_data(path = DATA_PATH):</span><br><span class="line">    csv_path = os.path.join(path,&quot;housing.csv&quot;)</span><br><span class="line">    return pd.read_csv(csv_path)</span><br></pre></td></tr></table></figure><p><a href="https://note.youdao.com/"> os.path.join( ) 用法详解</a></p><h2 id="快速查看数据结构"><a href="#快速查看数据结构" class="headerlink" title="快速查看数据结构"></a>快速查看数据结构</h2><ul><li>首先使用 head() 方法来查看数据开头的几行，大概浏览数据的特征以及每个特征的基本数值。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure><p><img src="/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ch_2_1.jpg" alt="图片无法显示"><br><img src="/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ch_2_1.jpg" class title="This is an test image"></p><ul><li>通过 info() 函数快速获得数据集的简单描述，主要是总行数以及每个属性类型非空值的数量。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.info())</span><br></pre></td></tr></table></figure><p><img src="/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ch_2_2.png" alt="图片无法显示"></p><ul><li>通过 value_counts() 方法查看有多少种分类存在。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[&#x27;ocean_proximity&#x27;].value_counts()</span><br></pre></td></tr></table></figure><p><img src="/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ch_2_3.png" alt="图片无法显示"></p><ul><li>通过 describe() 函数获得数值属性的摘要。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.describe()</span><br></pre></td></tr></table></figure><p><img src="/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ch_2_4.png" alt="图片无法显示"></p><ul><li>通过绘制每个数值属性的直方图来了解数据类型。</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.hist(bins=50, figsize=(20,15))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2021/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98-%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/ch_2_5.png" alt="图片无法显示"></p>]]></content>
      
      
      <categories>
          
          <category> 机器学习学习笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习实战 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
